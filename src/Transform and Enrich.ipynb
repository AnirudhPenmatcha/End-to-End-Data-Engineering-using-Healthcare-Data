{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51121680-3d12-4459-9975-22a645f16629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/anirudhp@megnity.com/healthcare_project/src/Explore and Clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4424c47f-1d0a-4364-910f-4d983e4ef069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_camp_cleaned.printSchema()\n",
    "second_camp_cleaned.printSchema()\n",
    "third_camp_cleaned.printSchema()\n",
    "patient_profiles_cleaned.printSchema()\n",
    "health_camp_details_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef86ee8b-27e5-4e73-b20f-41a1b26aa089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Custom framework to plan the transformations to be done \n",
    "\n",
    "1. There are 5 tables. See if there is any relationship between them. \n",
    "2. If there is a relationship, check if it makes sense to put the related tables together in whichever way it can be required for end user usecase\n",
    "3. If needed, create new calculated or derived field if needed\n",
    "4. The end outcome could be unified table or tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0e05f6-aab1-42e2-895b-21c9758ffad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The transformations that'll be done are the following:\n",
    "\n",
    "1. Combine the patient_id and health_camp_id from the camp tables into one dataframe. Then create a new column called camp_format and identify which format the unique combination of patient_id and health_camp_id falls under.\n",
    "2. Add the health scores for each unique combination of patient_id and health_camp_id. Note: the health score is only provided to people who attented either or both of the first two formats. \n",
    "3. Add other information such as donation for the people who attended the first camp format, stalls visited from the third camp format and other patient details from the table patient_profiles.\n",
    "4. Create a derived column of all the social media followers a patient has by totalling the followers they have across all the platforms they are in. Then remove the individual platform followers count columns. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d551bc2-a80f-4a14-b6e9-ef4a2e68efa1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752680761335}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1752680855218}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Union attendance DataFrames with Patient_ID, Health_Camp_ID, and Camp_Format\n",
    "try:\n",
    "    df_first_selected = first_camp_cleaned.select(\n",
    "        col(\"patient_id\"),\n",
    "        col(\"health_camp_id\"),\n",
    "        lit(\"First\").alias(\"Camp_Format\")\n",
    "    )\n",
    "    df_second_selected = second_camp_cleaned.select(\n",
    "        col(\"patient_id\"),\n",
    "        col(\"health_camp_id\"),\n",
    "        lit(\"Second\").alias(\"Camp_Format\")\n",
    "    )\n",
    "    df_third_selected = third_camp_cleaned.select(\n",
    "        col(\"patient_id\"),\n",
    "        col(\"health_camp_id\"),\n",
    "        lit(\"Third\").alias(\"Camp_Format\")\n",
    "    )\n",
    "\n",
    "    # Union the DataFrames\n",
    "    all_patients = df_first_selected.union(df_second_selected).union(df_third_selected)\n",
    "    print(\"Attendance DataFrames unified successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during union: {e}\")\n",
    "    raise\n",
    "\n",
    "display(all_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d410bc09-f4e9-49b9-b374-8b9b78845c07",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752681467844}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1752681637207}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine the health scores from the first_camp_cleaned and second_camp_cleaned\n",
    "\n",
    "combined_health_score = first_camp_cleaned.select(\n",
    "    first_camp_cleaned.columns[0],\n",
    "    first_camp_cleaned.columns[1],\n",
    "    first_camp_cleaned.columns[3]\n",
    ").union(\n",
    "    second_camp_cleaned.select(second_camp_cleaned.columns[:3])\n",
    ")\n",
    "\n",
    "# add all the health scores available to our all_patients table\n",
    "\n",
    "all_patients = all_patients.join(\n",
    "    combined_health_score.select(\"Patient_ID\", \"Health_Camp_ID\", \"health_score\"),\n",
    "    [\"Patient_ID\", \"Health_Camp_ID\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "display(all_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d163f481-61bf-4350-b8e5-f0d6be807b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add the donation information from first_camp_cleaned into all_patients\n",
    "\n",
    "all_patients = all_patients.join(\n",
    "    first_camp_cleaned.select(\n",
    "        col(\"Patient_ID\"),\n",
    "        col(\"Health_Camp_ID\"),\n",
    "        col(\"donation\")\n",
    "    ),\n",
    "    [\"Patient_ID\", \"Health_Camp_ID\"],\n",
    "    \"left\"\n",
    "    )\n",
    "    \n",
    "display(all_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b426fde-a7e0-40c7-9215-5402f5140cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add the number of stall visited and last stall visited number information from the third_camp_cleaned\n",
    "\n",
    "all_patients = all_patients.join(\n",
    "    third_camp_cleaned.select(\n",
    "        col(\"Patient_ID\"),\n",
    "        col(\"Health_Camp_ID\"),\n",
    "        col(\"number_of_stall_visited\"),\n",
    "        col(\"last_stall_visited_number\")\n",
    "    ),\n",
    "    [\"Patient_ID\", \"Health_Camp_ID\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "display(all_patients.filter(col(\"Camp_Format\") == \"Third\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cb87389-fa61-448b-9ac3-eca98a789a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join with patient_profile_clean for the rest of the patients' details\n",
    "\n",
    "all_patient_details = all_patients.join(\n",
    "    patient_profiles_cleaned,\n",
    "    \"Patient_ID\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "display(all_patient_details.limit(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c0cda1-3ee9-43e6-9d83-621e9c451e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# combine the columns online_follower, linkedin_shared, twitter_shared, facebook_shared into one column called 'total_social_media_followers' by adding the values horizontally. Then drop the columns online_follower, linkedin_shared, twitter_shared, facebook_shared\n",
    "\n",
    "all_patient_details = all_patient_details.withColumn(\n",
    "    \"total_social_media_followers\",\n",
    "    col(\"online_follower\") + col(\"linkedin_shared\") + col(\"twitter_shared\") + col(\"facebook_shared\")\n",
    ").drop(\n",
    "    \"online_follower\",\n",
    "    \"linkedin_shared\",\n",
    "    \"twitter_shared\",\n",
    "    \"facebook_shared\"\n",
    ")\n",
    "\n",
    "display(all_patient_details.limit(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ee95b4-b996-4faf-be27-9219d9c5bbd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, round\n",
    "\n",
    "cols_to_check = [\"income\", \"education_score\", \"age\", \"first_interaction\", \"city_type\", \"employer_category\"]\n",
    "total_rows = all_patient_details.count()\n",
    "\n",
    "null_counts = all_patient_details.select([\n",
    "    count(when(col(c).isNull(), c)).alias(f\"{c}_null_count\") for c in cols_to_check\n",
    "])\n",
    "\n",
    "null_percentages = null_counts.select([\n",
    "    (col(f\"{c}_null_count\") / total_rows * 100).alias(f\"{c}_null_pct\") for c in cols_to_check\n",
    "])\n",
    "\n",
    "\n",
    "display(null_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "774cdb4b-57e7-4ae6-946e-03bf719ff87f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How do I deal with the null values in the columns income, education_score, age, city_type, employer_category?\n",
    "\n",
    "# We could consider dropping columns with a high null value percentage but because we have a large enough dataset, even 20% of non-null data can be useful for all the above columns. So dropping columns here may not be the best approach. \n",
    "\n",
    "# Similarly if we need to consider dropping rows, the only columns that are critical would be the patient_id and health_camp_id columns which we have 0 null values in. So dropping rows here may not be the best approach either.\n",
    "\n",
    "# If we need to consider imputing values, ideally the null % should be quite less like 15% or less. So imputing values here may not be the best approach either.\n",
    "\n",
    "# Because it doesn't make clear sense to either drop rows/columns or impute values in our all_patient_details dataframe, we could keep the columns as it is and during analysis consider the null percentages and work with the dataframe as needed. \n",
    "\n",
    "\n",
    "# However, for the purposes of learning imputing, we will perform it here:\n",
    "'''\n",
    "- impute the null values in income with its median value. Income data is usually skewed, therefore using median would be better. Median is not affected by outliers.\n",
    "\n",
    "- impute the null values in education_score with its mean value. Education_score calculation hasn't been defined in the data description. Hence, we will assume that the data isn't skewed and will use mean. \n",
    "\n",
    "- impute the null values in age with its mean value. Generally, in some places you will see data being skewed in one direction for age. For example, Japan has a population with a lot of people in the older age group. Another example, would be India, where the population is skewed towards the younger age group. So if you had to randomly pick an individual from Japan, an educated guess would be that the person would be older. However, if you had to randomly pick an individual from India, an educated guess would be that the person would be younger. Hence, we will use mean for imputing age.\n",
    "\n",
    "- impute the null values in city_type with its mode value. The values in the city_type is categorical and we don't have much information to work with in the data description. So we will use mode for imputing city_type which is usually what is used for categorical data.\n",
    "\n",
    "- impute the null values in employer_category with the value 'Others'. Because we have very few values to work with and there is no one category where there is way too many people, we will fill it 'Others' for the purpose of this project\n",
    "\n",
    "- impute the null values in health_score with its mean value. Ideally the better thing to do for health_score would be to create a regression model and make it impute the values. But for the purposes of building an end-to-end data pipeline in practice, we will skip it for now. \n",
    "\n",
    "- impute the null values in donation with its median value. Mean would skewed similar to income. \n",
    "\n",
    "- impute the null values with 0 for number_of_stall_visited and last_stall_visited_number. This would be true because according to the data description, only the people who attended the third health camp format, visited stalls. It would make sense that others who never attended the third health camp format, would have visited no stalls.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12fe8d71-67b4-4548-8388-4601b0805d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# impute the null values in income with its median value\n",
    "\n",
    "income_median = all_patient_details.approxQuantile(\"income\", [0.5], 0.01)[0]\n",
    "all_patient_details = all_patient_details.fillna(income_median, subset=[\"income\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca75a29-5135-413d-afb0-93872b123731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# impute the null values in education_score with its mean value\n",
    "\n",
    "education_score_mean = all_patient_details.agg(avg(\"education_score\")).collect()[0][0]\n",
    "all_patient_details = all_patient_details.fillna(education_score_mean, subset=[\"education_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258259ae-37e6-46c7-84af-b31f3640accb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# impute the null values in age with its mean value.\n",
    "\n",
    "age_mean = all_patient_details.agg(avg(\"age\")).collect()[0][0]\n",
    "all_patient_details = all_patient_details.fillna(age_mean, subset=[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e120f53a-06d1-4b9d-b2a4-6e262f0e699c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# impute the null values in city_type with its mode value\n",
    "\n",
    "city_type_mode = all_patient_details.agg(mode(\"city_type\")).collect()[0][0]\n",
    "all_patient_details = all_patient_details.fillna(city_type_mode, subset=[\"city_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97ac4e3-915f-433f-8a18-7c56b39b0b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# impute the null values in employer_category with the value 'Others'\n",
    "\n",
    "all_patient_details = all_patient_details.fillna(\"Others\", subset=[\"employer_category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3fdd734-b7f7-411e-9604-3e42b1b58b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# impute the null values in health_score with its mean value\n",
    "\n",
    "health_score_mean = all_patient_details.agg(avg(\"health_score\")).collect()[0][0]\n",
    "all_patient_details = all_patient_details.fillna(health_score_mean, subset=[\"health_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756da264-8503-49a3-8953-8dddb593b8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# impute the null values in donation with its median value\n",
    "\n",
    "donation_median = all_patient_details.approxQuantile(\"donation\", [0.5], 0.01)[0]\n",
    "all_patient_details = all_patient_details.fillna(donation_median, subset=[\"donation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e740a0-dbe8-4543-a577-debbca1921f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# impute the null values with 0 for number_of_stall_visited and last_stall_visited_number\n",
    "\n",
    "all_patient_details = all_patient_details.fillna(0, subset=[\"number_of_stall_visited\", \"last_stall_visited_number\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7435101373746414,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform and Enrich",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
